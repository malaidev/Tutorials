{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quantization of TF model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTC1rDAuei_1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to lower precision numbers (int8, f16, etc.). This results in a smaller model and increased inferencing speed, which is valuable for low-power devices such as [microcontrollers](https://www.tensorflow.org/lite/microcontrollers). This data format is also required by integer-only accelerators such as the [Edge TPU](https://coral.ai/).\n",
        "\n",
        "In this tutorial, you'll train an MNIST model from scratch, convert it into a Tensorflow Lite file, and quantize it. Finally, you'll check the accuracy of the converted models and compare it to the original float model.\n",
        "\n",
        "You actually have several options as to how much you want to quantize a model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqqUIZjZjac"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0nR5AMEWq0H"
      },
      "source": [
        "In order to quantize model, we need to use APIs added in TensorFlow r2.3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsN6s5L1ieNl"
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "assert float(tf.__version__[:3]) >= 2.3"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XsEP17Zelz9"
      },
      "source": [
        "## Generate a TensorFlow Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NMaNZQCkW9X"
      },
      "source": [
        "We'll build a simple model to classify numbers from the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist).\n",
        "\n",
        "This training won't take long because you're training the model for just a 5 epochs, which trains to about ~98% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMsw_6HujaqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ed6e6c-4dbd-44d8-c734-0daf7256b06b"
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "test_images = test_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                  from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=5,\n",
        "  validation_data=(test_images, test_labels)\n",
        ")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.3000 - accuracy: 0.9148 - val_loss: 0.1467 - val_accuracy: 0.9582\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1255 - accuracy: 0.9637 - val_loss: 0.1046 - val_accuracy: 0.9681\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0902 - accuracy: 0.9736 - val_loss: 0.0777 - val_accuracy: 0.9753\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0720 - accuracy: 0.9790 - val_loss: 0.0731 - val_accuracy: 0.9776\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0609 - accuracy: 0.9823 - val_loss: 0.0629 - val_accuracy: 0.9815\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f753e418bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuTEoGFYd8aM"
      },
      "source": [
        "## Convert to a TensorFlow Lite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl8_fzVAZwOh"
      },
      "source": [
        "You can convert the trained model to TensorFlow Lite format using the [`TFLiteConverter`](https://www.tensorflow.org/lite/convert/python_api) API, and apply varying degrees of quantization.\n",
        "\n",
        "Beware that some versions of quantization leave some of the data in float format. So the following sections show each option with increasing amounts of quantization, until we get a model that's entirely int8 or uint8 data. (Notice we duplicate some code in each section so you can see all the quantization steps for each option.)\n",
        "\n",
        "First, here's a converted model with no quantization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i8B2nDZmAgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ea6336-2501-48e1-faec-0315f67719de"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "open(\"non-quant_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpfx1of4z5/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpfx1of4z5/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84576"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZocMZsA79ggO",
        "outputId": "8f841753-6204-435d-98cd-9cad40813cdd"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input type: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output type: ', output_type)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input type:  <class 'numpy.float32'>\n",
            "output type:  <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BONhYtYocQY"
      },
      "source": [
        "It's now a TensorFlow Lite model, but it's still using 32-bit float values for all parameter data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5U17DF7yZED"
      },
      "source": [
        "### **Technique-1: Post-training float16 quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Ud4Hn6yl_c"
      },
      "source": [
        "Weights are converted to 16-bit floating point values during model conversion from TensorFlow to TensorFlow Lite's flat buffer format. This results in a 2x reduction in model size. Some hardware, like GPUs, can compute natively in this reduced precision arithmetic, realizing a speedup over traditional floating point execution. The Tensorflow Lite GPU delegate can be configured to run in this way. However, a model converted to float16 weights can still run on the CPU without additional modification: the float16 weights are upsampled to float32 prior to the first inference. This permits a significant reduction in model size in exchange for a minimal impacts to latency and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtng-syY76DG"
      },
      "source": [
        "To quantize the model to float16 on export, first set the optimizations flag to use default optimizations. Then specify that float16 is the supported type on the target platform:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxelQWIcJIBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05337d85-8cf3-463b-9c71-aafd196ab0ae"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_fp16_model = converter.convert()\n",
        "open(\"post-training_f16_quant.tflite\", \"wb\").write(tflite_fp16_model)\n",
        "tflite_model_fp16_file = tflite_models_dir/\"post-training_f16_quant.tflite\"\n",
        "model_size = tflite_model_fp16_file.write_bytes(tflite_fp16_model)/1000\n",
        "print(\"Model size after quantization: %s Kb\"%model_size)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8dfd2u30/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8dfd2u30/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size after quantization: 44.528 Kb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-w0K8bI9HMC",
        "outputId": "c0485035-8b34-4280-c8c2-54eb1e3bcf46"
      },
      "source": [
        "interpreter_fp16 = tf.lite.Interpreter(model_path=str(tflite_model_fp16_file))\n",
        "interpreter_quant.allocate_tensors()\n",
        "input_type = interpreter_fp16.get_input_details()[0]['dtype']\n",
        "print('input type: ', input_type)\n",
        "output_type = interpreter_fp16.get_output_details()[0]['dtype']\n",
        "print('output type: ', output_type)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input type:  <class 'numpy.float32'>\n",
            "output type:  <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPYZwgZTwJMT"
      },
      "source": [
        "### **Technique-2: Post-training dynamic range quantization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjvq1vpJd4U_"
      },
      "source": [
        "Now let's enable the default `optimizations` flag to quantize all fixed parameters (such as weights). The activations are always stored in floating point. For ops that support quantized kernels, the activations are quantized to 8 bits of precision dynamically prior to processing and are de-quantized to float precision after processing. Depending on the model being converted, this can give a speedup over pure floating point computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEZ6ET1AHAS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e1ac25-7ce1-48d4-82d0-90ac19b893da"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "dynamic_quant = converter.convert()\n",
        "open(\"post-training_dynamic_quant.tflite\", \"wb\").write(dynamic_quant)\n",
        "dynamic_quant_file = tflite_models_dir/\"post-training_dynamic_quant.tflite\"\n",
        "model_size = dynamic_quant_file.write_bytes(dynamic_quant)/1000\n",
        "print(\"Model size after quantization: %s Kb\"%model_size)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp7zg8yi40/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp7zg8yi40/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size after quantization: 23.984 Kb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b388c3Bu87D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130f7ff1-3a84-470e-81d6-cbd799410085"
      },
      "source": [
        "interpreter_quant = tf.lite.Interpreter(model_path=str(dynamic_quant_file))\n",
        "interpreter_quant.allocate_tensors()\n",
        "input_type = interpreter_quant.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter_quant.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  <class 'numpy.float32'>\n",
            "output:  <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5wuE-RcdX_3"
      },
      "source": [
        "The model is now a bit smaller with quantized weights, but other variable data is still in float format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Teu7zL-Q5e"
      },
      "source": [
        "### **Technique-3: Post-training integer quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s76APeJk-XEM"
      },
      "source": [
        "Integer quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to the nearest 8-bit fixed-point numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgKDdnHQEhpb"
      },
      "source": [
        "1) **Convert using float fallback quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTe8avZJHMDO"
      },
      "source": [
        "To quantize the variable data (such as model input/output and intermediates between layers), you need to provide a [`RepresentativeDataset`](https://www.tensorflow.org/api_docs/python/tf/lite/RepresentativeDataset). This is a generator function that provides a set of input data that's large enough to represent typical values. It allows the converter to estimate a dynamic range for all the variable data. (The dataset does not need to be unique compared to the training or evaluation dataset.)\n",
        "To support multiple inputs, each representative data point is a list and elements in the list are fed to the model according to their indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiwiWU3gHdkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09aecaf5-8855-48eb-f27d-26faca3585ed"
      },
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "tflite_quant_float = converter.convert()\n",
        "open(\"quant_float.tflite\", \"wb\").write(tflite_quant_float)\n",
        "tflite_quant_float_file = tflite_models_dir/\"quant_float.tflite\"\n",
        "model_size = tflite_quant_float_file.write_bytes(tflite_quant_float)/1000\n",
        "print(\"Model size after quantization: %s Kb\"%model_size)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpmeun8087/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpmeun8087/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size after quantization: 24.296 Kb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GC3HFlptf7x"
      },
      "source": [
        "Now all weights and variable data are quantized, and the model is significantly smaller compared to the original TensorFlow Lite model.\n",
        "\n",
        "However, to maintain compatibility with applications that traditionally use float model input and output tensors, the TensorFlow Lite Converter leaves the model input and output tensors in float:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id1OEKFELQwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58504494-74d5-4c68-f835-c5dfe243d2d3"
      },
      "source": [
        "interpreter_quant_float = tf.lite.Interpreter(model_path=str(tflite_model_quant_float_file))\n",
        "interpreter_quant_float.allocate_tensors()\n",
        "input_type = interpreter_quant_float.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter_quant_float.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  <class 'numpy.float32'>\n",
            "output:  <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RACBJuj2XO8x"
      },
      "source": [
        "That's usually good for compatibility, but it won't be compatible with devices that perform only integer-based operations, such as the Edge TPU.\n",
        "\n",
        "Additionally, the above process may leave an operation in float format if TensorFlow Lite doesn't include a quantized implementation for that operation. This strategy allows conversion to complete so you have a smaller and more efficient model, but again, it won't be compatible with integer-only hardware. (All ops in this MNIST model have a quantized implementation.)\n",
        "\n",
        "So to ensure an end-to-end integer-only model, you need a couple more parameters..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQgTqbvPvxGJ"
      },
      "source": [
        "2) **Convert using integer-only quantization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwR9keYAwArA"
      },
      "source": [
        "To quantize the input and output tensors, and make the converter throw an error if it encounters an operation it cannot quantize, convert the model again with some additional parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzjEjcDs3BHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afaafe81-fe13-4b8e-c548-a47900cda413"
      },
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_quant_int = converter.convert()\n",
        "open(\"quant_int.tflite\", \"wb\").write(tflite_quant_int)\n",
        "tflite_model_quant_int_file = tflite_models_dir/\"quant_int.tflite\"\n",
        "model_size = tflite_model_quant_int_file.write_bytes(tflite_quant_int)/1000\n",
        "print(\"Model size after quantization: %s Kb\"%model_size)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpvdu7nbpw/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpvdu7nbpw/assets\n",
            "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size after quantization: 24.352 Kb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYd6NxD03yjB"
      },
      "source": [
        "The internal quantization remains the same as above, but you can see the input and output tensors are now integer format:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaNkOS-twz4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb84752-c6e8-407b-d6f4-12c712e5a207"
      },
      "source": [
        "interpreter_quant_int = tf.lite.Interpreter(model_path=str(tflite_model_quant_int_file))\n",
        "interpreter_quant_int.allocate_tensors()\n",
        "input_type = interpreter_quant_int.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter_quant_int.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  <class 'numpy.uint8'>\n",
            "output:  <class 'numpy.uint8'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO17AP84wzBb"
      },
      "source": [
        "Now you have an integer quantized model that uses integer data for the model's input and output tensors, so it's compatible with integer-only hardware such as the [Edge TPU](https://coral.ai)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c39DNS7IiXcG"
      },
      "source": [
        "### **Technique-4: Quantization Aware Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vXmnSBajnNZ"
      },
      "source": [
        "Clone and fine-tune pre-trained model with quantization aware training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpmYJ1kTmvxB"
      },
      "source": [
        "**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZwuEZbomuyb"
      },
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM54LOZpj2rR"
      },
      "source": [
        "**Define the model**\n",
        "\n",
        "You will apply quantization aware training to the whole model and see this in the model summary. All layers are now prefixed by \"quant\".\n",
        "\n",
        "Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8). The sections after show how to create a quantized model from the quantization aware one.\n",
        "\n",
        "In the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md), you can see how to quantize some layers for model accuracy improvements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdMTEiQwjbfT",
        "outputId": "17f2bc6c-e52d-4919-eb4c-4b8dff8e520c"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quantize_layer_3 (QuantizeLa (None, 28, 28)            3         \n",
            "_________________________________________________________________\n",
            "quant_reshape_3 (QuantizeWra (None, 28, 28, 1)         1         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_3 (QuantizeWrap (None, 26, 26, 12)        147       \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_3 (Quant (None, 13, 13, 12)        1         \n",
            "_________________________________________________________________\n",
            "quant_flatten_3 (QuantizeWra (None, 2028)              1         \n",
            "_________________________________________________________________\n",
            "quant_dense_3 (QuantizeWrapp (None, 10)                20295     \n",
            "=================================================================\n",
            "Total params: 20,448\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 38\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRh5C-3kI2J"
      },
      "source": [
        "**Train and evaluate the model against baseline**\n",
        "\n",
        "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QRpnMEtkMa-",
        "outputId": "842dadda-3d20-4cad-ed5a-d0c9131cbd7c"
      },
      "source": [
        "train_images_subset = train_images[0:1000] # out of 60000\n",
        "train_labels_subset = train_labels[0:1000]\n",
        "\n",
        "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=500, epochs=1, validation_split=0.1)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 301ms/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0660 - val_accuracy: 0.9800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74e4eac9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENvkExL-krgw"
      },
      "source": [
        "**Create quantized model for TFLite backend**\n",
        "\n",
        "After this, you have an actually quantized model with int8 weights and uint8 activations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUSl6gUvkpJU",
        "outputId": "a19e704f-d898-4c8c-ba9a-6e5ac5bdd263"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quant_aware_training = converter.convert()\n",
        "open(\"quant_aware_training.tflite\", \"wb\").write(quant_aware_training)\n",
        "quant_aware_training_file = tflite_models_dir/\"quant_aware_training.tflit\"\n",
        "model_size = quant_aware_training_file.write_bytes(quant_aware_training)/1000\n",
        "print(\"Model size after quantization: %s Kb\"%model_size)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as reshape_3_layer_call_and_return_conditional_losses, reshape_3_layer_call_fn, conv2d_3_layer_call_and_return_conditional_losses, conv2d_3_layer_call_fn, flatten_3_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpi8hakt7x/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpi8hakt7x/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size after quantization: 24.688 Kb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t9yaTeF9fyM"
      },
      "source": [
        "## Run the TensorFlow Lite models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8lQHMp_asCq"
      },
      "source": [
        "Now we'll run inferences using the TensorFlow Lite [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) to compare the model accuracies.\n",
        "\n",
        "First, we need a function that runs inference with a given model and images, and then returns the predictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X092SbeWfd1A"
      },
      "source": [
        "# Helper function to run inference on a TFLite model\n",
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_images\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = test_images[test_image_index]\n",
        "    test_label = test_labels[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "\n",
        "  return predictions\n"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwN7uIdCd8Gw"
      },
      "source": [
        "### Evaluate the models on all images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFKOD4DG8XmU"
      },
      "source": [
        "Now let's run both models using all the test images we loaded at the beginning of this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05aeAuWjvjPx"
      },
      "source": [
        "# Helper function to evaluate a TFLite model on all images\n",
        "def evaluate_model(tflite_file, model_type):\n",
        "  global test_images\n",
        "  global test_labels\n",
        "\n",
        "  test_image_indices = range(test_images.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "\n",
        "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
        "\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
        "      model_type, accuracy, len(test_images)))"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP0rxDpopnQd"
      },
      "source": [
        "**Technique-1 Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtyRS87TS4XO",
        "outputId": "fd83b375-197f-49ad-8410-26dcd531f3dc"
      },
      "source": [
        "evaluate_model(str(tflite_model_fp16_file), model_type=\"post-training_f16_quant.tflite\")"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "post-training_f16_quant.tflite model accuracy is 98.1500% (Number of test samples=10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnFilQpBuMh5"
      },
      "source": [
        "**Technique-2 Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vLNC2_upyEB",
        "outputId": "11aaf975-fb66-4c04-b9c6-44312d7c2b53"
      },
      "source": [
        "evaluate_model(str(dynamic_quant_file), model_type=\"post-training_dynamic_quant.tflite\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "post-training_dynamic_quant.tflite model accuracy is 98.1400% (Number of test samples=10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLWIlqBzqD5n"
      },
      "source": [
        "**Technique-3 Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5mWkSbMcU5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0d77a1-ad5e-4416-98c7-053833b5b2b1"
      },
      "source": [
        "evaluate_model(str(tflite_model_quant_float_file), model_type=\"quant_float.tflite\")"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quant_float.tflite model accuracy is 97.8600% (Number of test samples=10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9cnwiPp6EGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fd01a2-298b-46bf-c05d-1f1b216997aa"
      },
      "source": [
        "evaluate_model(str(tflite_model_quant_int_file), model_type=\"quant_int.tflite\")"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quant_int.tflite model accuracy is 98.0300% (Number of test samples=10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgBZ2oBpqOrL"
      },
      "source": [
        "**Technique-4 Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGGJov3IqLxX",
        "outputId": "b66d5e47-e28d-4fbd-dd45-382b483eac32"
      },
      "source": [
        "evaluate_model(str(quant_aware_training_file), model_type=\"quant_aware_training.tflite\")"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quant_aware_training.tflite model accuracy is 98.1000% (Number of test samples=10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lfxkor8pgv"
      },
      "source": [
        "We can't really compare the accuracy of different quantization techniques. Though quantization aware training is often better for model accuracy. \n",
        "\n",
        "To learn more about other quantization strategies, read about [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization)."
      ]
    }
  ]
}